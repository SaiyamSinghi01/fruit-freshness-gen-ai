# -*- coding: utf-8 -*-
"""fruit_classifier_training(classifier+augmentation+GAN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EExLu1tY17cGynxH-357Tz9orwKB7AeU
"""

!pip install -q torch torchvision torchaudio
!pip install -q diffusers transformers accelerate safetensors
!pip install -q pillow tqdm matplotlib scikit-learn seaborn fpdf
!pip install -q pytorch-lightning

!pip install -q imageio-ffmpeg

import os, shutil, time, random, re
from pathlib import Path
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import random_split, DataLoader, Dataset
import torchvision
from torchvision import transforms, datasets, utils, models
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

from google.colab import drive


ROOT = Path("/content/banana")
IMG_DIR = ROOT / "clean_dataset"

SYN_DIR = ROOT / "synthetic"
MODEL_DIR = ROOT / "models"
for p in [ROOT, IMG_DIR, SYN_DIR, MODEL_DIR]:
    p.mkdir(parents=True, exist_ok=True)

print("Root:", ROOT)

import os

print("Folders in IMG_DIR:", os.listdir(IMG_DIR))
for folder in os.listdir(IMG_DIR):
    path = os.path.join(IMG_DIR, folder)
    print(f"{folder}: {len(os.listdir(path))} images")

import os, shutil, random, time
from pathlib import Path
import torch
from torchvision import transforms, utils, datasets
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

ROOT = Path("/content/banana")
IMG_DIR = ROOT / "clean_dataset"      # original dataset with 4 class subfolders
AUG_DIR = ROOT / "augmented_dataset"  # where augmented + real will go
SYN_DIR = ROOT / "synthetic"          # where GAN will save synthetic
MODEL_DIR = ROOT / "models"
for p in [ROOT, IMG_DIR, AUG_DIR, SYN_DIR, MODEL_DIR]:
    p.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)
print("Original classes:", sorted([d.name for d in IMG_DIR.iterdir() if d.is_dir()]))

import torchvision.transforms.functional as TF
from torchvision import transforms
import uuid

AUG_FACTOR = 10   # produce ~AUG_FACTOR versions per original image (including original)
IMG_SIZE = 128    # final size for GAN

base_aug = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),

])


def random_aug(image):

    # random crop + rotate + color jitter + gaussian noise optionally
    if random.random() < 0.5:
        image = TF.hflip(image)
    if random.random() < 0.3:
        image = TF.vflip(image)
    if random.random() < 0.6:
        angle = random.uniform(-25, 25)
        image = image.rotate(angle)
    if random.random() < 0.6:
        # color jitter
        cj = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.25, hue=0.05)
        image = cj(image)
    if random.random() < 0.4:
        # small affine
        image = TF.affine(image, angle=0, translate=(random.randint(-5,5), random.randint(-5,5)), scale=1.0+random.uniform(-0.05,0.05), shear=random.uniform(-5,5))

    if random.random() < 0.3:
        arr = np.array(image).astype(np.float32)
        noise = np.random.normal(0, 8, arr.shape)
        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)
        image = Image.fromarray(arr)
    # final resize to IMG_SIZE
    image = image.resize((IMG_SIZE, IMG_SIZE))
    return image

# create augmented dataset folders and copy originals
for cls in sorted([d.name for d in IMG_DIR.iterdir() if d.is_dir()]):
    src = IMG_DIR / cls
    dst = AUG_DIR / cls
    if dst.exists():
        shutil.rmtree(dst)
    dst.mkdir(parents=True, exist_ok=True)
    files = list(src.glob("*"))
    print(f"{cls}: {len(files)} originals")
    # copy originals
    for i, f in enumerate(files):
        try:
            img = Image.open(f).convert("RGB")
        except:
            continue
        outname = dst / f"{f.stem}_orig{f.suffix}"
        img.resize((IMG_SIZE, IMG_SIZE)).save(outname)
        # augment variations
        for k in range(AUG_FACTOR-1):
            aug_img = random_aug(img)
            outname = dst / f"{f.stem}_aug{k:02d}{f.suffix}"
            aug_img.save(outname)

for cls in sorted([d.name for d in AUG_DIR.iterdir() if d.is_dir()]):
    print(cls, "->", len(list((AUG_DIR/cls).glob("*"))), "images")

!rm -rf /content/banana/clean_dataset/.ipynb_checkpoints

import shutil

for root, dirs, files in os.walk(AUG_DIR):
    if ".ipynb_checkpoints" in dirs:
        chk = os.path.join(root, ".ipynb_checkpoints")
        print("Removing:", chk)
        shutil.rmtree(chk)

import os
for cls in os.listdir(AUG_DIR):
    print(cls, len(os.listdir(AUG_DIR/cls)))

import os, shutil
from pathlib import Path

ROOT = Path("/content/banana")
AUG_DIR = ROOT / "augmented_dataset"
SYN_DIR = ROOT / "synthetic"
MODEL_DIR = ROOT / "models"


for r, d, f in os.walk(AUG_DIR):
    if ".ipynb_checkpoints" in d:
        shutil.rmtree(os.path.join(r, ".ipynb_checkpoints"))


for cls in sorted([p.name for p in AUG_DIR.iterdir() if p.is_dir()]):
    print(cls, "->", len(list((AUG_DIR/cls).glob("*"))))



print("SYN_DIR:", SYN_DIR)
print("Classes:", class_names)

#GAN
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, utils
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from PIL import Image
import os, shutil
from tqdm import tqdm

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", DEVICE)


ROOT = Path("/content/banana")
IMG_DIR = ROOT / "clean_dataset"
SYN_DIR = ROOT / "synthetic"
if SYN_DIR.exists():
    shutil.rmtree(SYN_DIR)
SYN_DIR.mkdir(parents=True, exist_ok=True)


IMG_SIZE = 32
BATCH = 16
nz = 64
num_classes = 4            # overripe, ripe, rotten, unripe
embedding_dim = 16
ngf = 32
ndf = 32
lambda_gp = 10
EPOCHS = 150



transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])


dataset = datasets.ImageFolder(str(IMG_DIR), transform=transform)
loader = DataLoader(dataset, batch_size=BATCH, shuffle=True, drop_last=True, num_workers=2)

class_names = dataset.classes
print("Classes:", class_names)


#Conditional Generator

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, embedding_dim)

        self.fc = nn.Sequential(
            nn.Linear(nz + embedding_dim, ngf * 8 * 4 * 4),
            nn.ReLU(True),
        )

        self.main = nn.Sequential(
            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf*4),
            nn.ReLU(True),

            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf*2),
            nn.ReLU(True),

            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            nn.ConvTranspose2d(ngf, 3, 3, 1, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        label_emb = self.label_emb(labels)
        x = torch.cat([noise, label_emb], dim=1)
        x = self.fc(x)
        x = x.view(-1, ngf*8, 4, 4)
        return self.main(x)


# Conditional Discriminator

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.label_emb = nn.Embedding(num_classes, IMG_SIZE * IMG_SIZE)

        self.main = nn.Sequential(
            nn.Conv2d(4, ndf, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf, ndf*2, 4, 2, 1),
            nn.InstanceNorm2d(ndf*2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1),
            nn.InstanceNorm2d(ndf*4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ndf*4, 1, 4, 1, 0)
        )

    def forward(self, img, labels):
        label_map = self.label_emb(labels)
        label_map = label_map.view(-1, 1, IMG_SIZE, IMG_SIZE)
        x = torch.cat([img, label_map], dim=1)
        return self.main(x).view(-1)


netG = Generator().to(DEVICE)
netD = Discriminator().to(DEVICE)

optG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.0, 0.9))
optD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.0, 0.9))

print("Models Ready")

#Penalty
def gradient_penalty(D, real, fake, labels):
    alpha = torch.rand(real.size(0), 1, 1, 1, device=DEVICE)
    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)
    out = D(interpolated, labels)

    gradients = torch.autograd.grad(
        outputs=out, inputs=interpolated,
        grad_outputs=torch.ones_like(out),
        create_graph=True, retain_graph=True
    )[0]

    gradients = gradients.view(real.size(0), -1)
    return ((gradients.norm(2, dim=1) - 1)**2).mean()

def train():
    for epoch in range(EPOCHS):
        d_loss_epoch, g_loss_epoch = 0, 0

        for real_imgs, labels in tqdm(loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
            real_imgs, labels = real_imgs.to(DEVICE), labels.to(DEVICE)
            b = real_imgs.size(0)

            # Train D
            noise = torch.randn(b, nz, device=DEVICE)
            fake_imgs = netG(noise, labels).detach()

            real_score = netD(real_imgs, labels)
            fake_score = netD(fake_imgs, labels)
            gp = gradient_penalty(netD, real_imgs, fake_imgs, labels)

            d_loss = -(real_score.mean() - fake_score.mean()) + lambda_gp * gp

            optD.zero_grad()
            d_loss.backward()
            optD.step()

            # Train G
            noise = torch.randn(b, nz, device=DEVICE)
            fake_imgs = netG(noise, labels)

            g_loss = -netD(fake_imgs, labels).mean()

            optG.zero_grad()
            g_loss.backward()
            optG.step()

            d_loss_epoch += d_loss.item()
            g_loss_epoch += g_loss.item()

        print(f"Epoch {epoch+1}: D_loss={d_loss_epoch:.3f} | G_loss={g_loss_epoch:.3f}")


        with torch.no_grad():
            sample_labels = torch.tensor([0,1,2,3], device=DEVICE)
            noise = torch.randn(4, nz, device=DEVICE)
            samples = netG(noise, sample_labels).cpu()
            grid = utils.make_grid((samples+1)/2, nrow=4)
            plt.imshow(grid.permute(1,2,0))
            plt.axis("off")
            plt.show()

train()


def generate_images(n_per_class=100):
    netG.eval()
    for cls_idx, cls in enumerate(class_names):
        folder = SYN_DIR / cls
        folder.mkdir(exist_ok=True, parents=True)

        for i in range(n_per_class):
            noise = torch.randn(1, nz, device=DEVICE)
            label = torch.tensor([cls_idx], device=DEVICE)
            fake = netG(noise, label)[0].cpu()
            img = transforms.ToPILImage()( (fake+1)/2 )
            img.save(folder / f"{cls}_{i:04d}.png")

    print("Done generating synthetic images!")

generate_images(150)

FINAL = ROOT / "final_dataset"
if FINAL.exists(): shutil.rmtree(FINAL)
shutil.copytree(str(AUG_DIR), str(FINAL))


for cls in class_names:
    dst = FINAL/cls
    for f in (SYN_DIR/cls).glob("*"):
        shutil.copy(str(f), str(dst))

import os
from pathlib import Path

ROOT = Path("/content/banana")
FINAL_DIR = ROOT / "final_dataset"

# Remove any .ipynb_checkpoints
for r, d, f in os.walk(FINAL_DIR):
    if ".ipynb_checkpoints" in d:
        shutil.rmtree(os.path.join(r, ".ipynb_checkpoints"))

# Count images in each class folder
for cls in sorted([p.name for p in FINAL_DIR.iterdir() if p.is_dir()]):
    count = len(list((FINAL_DIR / cls).glob("*")))
    print(cls, "->", count)

import torch.nn as nn, torch.optim as optim
from torchvision import models
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# hyperparams
NUM_EPOCHS = 20
BATCH_CLS = 32
IMG_CLS = 224
LR = 1e-4


train_t = transforms.Compose([
    transforms.RandomResizedCrop(IMG_CLS),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
val_t = transforms.Compose([
    transforms.Resize((IMG_CLS,IMG_CLS)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

full = datasets.ImageFolder(str(FINAL), transform=train_t)
# split train/val/test 80/10/10
n = len(full)
n_train = int(0.8*n)
n_val = int(0.1*n)
n_test = n - n_train - n_val
train_ds, val_ds, test_ds = torch.utils.data.random_split(full, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42))

val_ds.dataset = datasets.ImageFolder(str(FINAL), transform=val_t)
test_ds.dataset = datasets.ImageFolder(str(FINAL), transform=val_t)

train_loader = DataLoader(train_ds, batch_size=BATCH_CLS, shuffle=True, num_workers=2)
val_loader = DataLoader(val_ds, batch_size=BATCH_CLS, shuffle=False, num_workers=2)
test_loader = DataLoader(test_ds, batch_size=BATCH_CLS, shuffle=False, num_workers=2)

# model
model = models.mobilenet_v2(pretrained=True)
model.classifier[1] = nn.Linear(model.last_channel, len(class_names))
model = model.to(device)

#  (freeze backbone)
for p in model.features.parameters():
    p.requires_grad = False

criterion = nn.CrossEntropyLoss()
opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)

best_val = 0.0
for epoch in range(1, NUM_EPOCHS+1):
    model.train()
    running_loss = 0.0
    correct = total = 0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        opt.zero_grad()
        out = model(imgs)
        loss = criterion(out, labels)
        loss.backward()
        opt.step()
        running_loss += loss.item()*imgs.size(0)
        preds = out.argmax(dim=1)
        correct += (preds==labels).sum().item()
        total += imgs.size(0)
    train_acc = correct/total
    # val
    model.eval()
    vcorrect = vtotal = 0
    vloss = 0.0
    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            out = model(imgs)
            loss = criterion(out, labels)
            vloss += loss.item()*imgs.size(0)
            preds = out.argmax(dim=1)
            vcorrect += (preds==labels).sum().item()
            vtotal += imgs.size(0)
    val_acc = vcorrect/vtotal
    print(f"[{epoch}/{NUM_EPOCHS}] train_acc {train_acc:.4f} val_acc {val_acc:.4f} loss {running_loss/total:.4f}")
    if val_acc > best_val:
        best_val = val_acc
        torch.save(model.state_dict(), MODEL_DIR / "mobilenet_final.pth")

# Evaluate on test set
model.load_state_dict(torch.load(MODEL_DIR/"mobilenet_final.pth"))
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for imgs, labels in test_loader:
        imgs = imgs.to(device)
        out = model(imgs)
        preds = out.argmax(dim=1).cpu().numpy()
        y_pred.extend(preds.tolist())
        y_true.extend(labels.numpy().tolist())

print("Classification report:")
print(classification_report(y_true, y_pred, target_names=class_names))
# confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("pred"); plt.ylabel("true"); plt.show()